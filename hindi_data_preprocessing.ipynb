{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing:\n",
    "### Contributed by Parth Wadhwa, [GitHub Repo](https://github.com/fnc11/nball4treehindi)\n",
    "\n",
    "Files used for Hindi data generation are taken from this [github repo](https://bitbucket.org/sivareddyg/python-hindi-wordnet/src/master) which mainly took data from [IIT Bombay University](http://www.cfilt.iitb.ac.in/).\n",
    "\n",
    "You need to download w2v from this [website](https://fasttext.cc/docs/en/crawl-vectors.html) and make sure you remove first line of this file as it contains information about number of words and dimensions.\n",
    "\n",
    "\n",
    "## Extract Embedding words:\n",
    "First extract words from word2vector file, so that we can filter out the word from our database for which we don't have the vectors.\n",
    "    * since the size of w2v file is large(4 GB), it's already done here, you can find the extracted words in data folder.\n",
    "    * keep in mind this step is very computation intensive so only run it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cc.hi.300.vec\",'r') as vec, open(\"data/wordEmbs.txt\",'w') as word_embs:\n",
    "    cont = vec.read()\n",
    "    lines = cont.split('\\n')\n",
    "    for line in lines:\n",
    "        word = line.split(\" \")[0]\n",
    "        word_embs.write(word+\"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download these three packages from the mentioned [repository](https://bitbucket.org/sivareddyg/python-hindi-wordnet/src/master/)\n",
    "    * WordSynsetDict.pk, SynsetHypernym.pk, SynsetWords.pk(this can be skipped). These are also available in data folder.\n",
    "## Data Format\n",
    "Here is a description of the data format of these packages and what we tried to achieve from this data.\n",
    "For this, we used WordSynsetDict.pk and SynsetHypernym.pk dictionaries which we got from Siva Reddy’s repository. Now, these files contain a structure like the following:\n",
    "WordSynsetDict.pk\n",
    "Actual:\n",
    "नरहरि\n",
    "{'1': [u'19440']}\n",
    "झौंस\n",
    "{'1': [u'12911', u'8884'], '3': [u'11454', u'8391']}\n",
    "सेनजित्\n",
    "{'1': [u'21358', u'21357']}\n",
    "\n",
    "English Analogy:\n",
    "word1\n",
    "{'1': [u'set19440']}\n",
    "word2\n",
    "{'1': [u'set12911', u'set8884'], '3': [u'set11454', u'set8391']}\n",
    "…\n",
    "So, words are dictionary keys which also stores another dictionary as value, inside the value dictionary another dictionary which stores a list of sets according to the noun, adjective, verb and adverb as 1,2,3,4 respectively, which contains this word in their set. It’s pretty complicated to understand but by analysing an example perhaps you could understand more clearly.\n",
    "\n",
    "SynsetHypernym.pk\n",
    "{'12836': {1: [u'196']}, '12835': {1: [u'1070']}, '12834': {1: [u'652']}, '8545': {1: [u'1439', u'7290']}, '8544': {1: [u'564']}, '12839': {3: [u'13028']}, '8546': {1: [u'3139']}, '8541': {3: [u'5367']}..........}\n",
    "\n",
    "Now, this is also a dictionary where set numbers are the keys, and they hold also a dictionary as their value, this value dictionary stores the immediate parent sets of the current set. But they are not stored as a list, the parents are also stored as a dictionary which stores the parents according to the set’s type(noun(1), adjective(2), verb(3) and adverb(4)). I know that you would think about why there are different parent sets for one set. But the data is in this format, maybe there are cases that a set contains a word which behaves as noun and verb depending upon the context, then the parent sets of this set in which this word resides could have multiple types of parent sets like noun as well as the verb.\n",
    "\n",
    "Our main goal is to produce the data in such a format which can be ultimately used for generating data which can be fed to the nball4tree code. So for this, we tried to print the paths from word to root or the last parent they had so that in the next section we could easily just take these paths and build a tree.\n",
    "“Just keep in mind in this context we use set/synset very frequently, often they mean the same thing here, but we’ll make sure to let you know if they meant otherwise.”\n",
    "So the first thing we did was to filter out the sets which contained all the words which are not present in the word embeddings or for them we didn’t have the embeddings, while doing this operation we had to keep in my mind that we don’t remove a set if it contains some words for which we do have embeddings. And we made a set which contained all the synset numbers which need to be removed entirely from the data, we didn’t remove these sets here right away because doing so might break the hierarchical structure of many words instead we remove them from the final paths.\n",
    "And since we had several words inside one set, we numbered them according to the word type and it’s number in the set for example:\n",
    "झौंस\n",
    "{'1': [u'12911', u'8884'], '3': [u'11454', u'8391']}\n",
    "\n",
    "झौंस.n.01 -> 12911\n",
    "झौंस.n.02 -> 8884\n",
    "झौंस.v.01 -> 11454\n",
    "झौंस.v.02 -> 8391\n",
    "\n",
    "And we also made a reverse dictionary(modernSyn2Words) where sets were the keys and which held a list of words as their value like the following:\n",
    "{‘12911’:[झौंस.n.01], ‘8884’:[झौंस.n.02], …}\n",
    "We need this type of dictionary for later purposes since this each set in this dictionary containing a list of words we just printed only the first word in each list corresponding to a set, this was some kind of optimization approach we took, which will be explained later.\n",
    "Now after formatting the words, it was time to print the paths which we did use the hypernym relation between the sets, yes we didn’t have the hypernym relation for a particular word but for the entire set. So we did the only sensible thing anyone would do, we kept the sets in leaves to root paths. So our paths looked like this:  \n",
    "स्वरभंग.n.01<-17449<-1423<-652\\\\$  \n",
    "नाइजीरिया.n.01<-20242<-7440<-3108<-2022<-923<-3259\\\\$  \n",
    "….  \n",
    "So that at later of point of time how to deal with this and also kept more information this way, as we said above we removed the sets for which we didn’t have embeddings finally here after gettings these paths.\n",
    "\n",
    "## Difficulties/Choices:\n",
    "1. The first difficulty we face was with the computation resources, the w2v file was almost around 4GB size so need to extract the words from it separately so that we can run the processes efficiently. \n",
    "2. In the first section when we had to make the paths for the nodes, we find there are some branch splitting/merging data exist, like a single set have multiple parents and they go on separate paths and later they merge on some same node. So for simplicity, we just took the first parent set number from each set of hypernym sets to make it work.\n",
    "3. Keeping the set number on intermediate nodes instead of the actual words.\n",
    "4. Deferring the decision to remove the sets to later point from the paths, found out after some tries.\n",
    "\n",
    "\n",
    "## Run the below code stepwise it will generate tree structure or paths from leaves to last parent.\n",
    "\n",
    "### Read the packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Loading input dictionaries\n",
    "word2synsets = pickle.load(open(\"data/WordSynsetDict.pk\", 'rb'))\n",
    "# synset2words = pickle.load(open(\"data/SynsetWords.pk\", 'rb'))\n",
    "synset2hypes = pickle.load(open(\"data/SynsetHypernym.pk\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the paths from the leaf/word to root will be printed in tree_struct.txt\n",
    "tree_struct = open(\"data/tree_struct.txt\", 'w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning words according to their type \n",
    "A function to assign characters based upon the word type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function assigns one special char to each word type in the wordnet. \n",
    "    1: n (noun), 2: j (Adjective), 3: v (verb), 4: a (Adverb)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def assign_alpha(num):\n",
    "    if num == \"1\":\n",
    "        return \"n\"\n",
    "    elif num == \"2\":\n",
    "        return \"j\"\n",
    "    elif num == \"3\":\n",
    "        return \"v\"\n",
    "    else:\n",
    "        return \"r\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the sets\n",
    "* Make two sets to_remove and to_keep which holds the synsets which we need to remove and keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Compares the words from the Hindi Wordnet and the Hindi Word-embeddings, and keeps track of the synsets which \n",
    "contains the words which are not in the word-embeddings so that they can be removed later from the Hindi Wordnet.\n",
    "\"\"\"\n",
    "\n",
    "# Synsets of the words which are not in the word-embeddings file.\n",
    "to_remove = set()\n",
    "\n",
    "# Synsets of the commen words between the both files.\n",
    "to_keep = set()\n",
    "\n",
    "# embwords.txt contains all the words which are present in the Hindi Word embeddings file\n",
    "# sets2remove.txt contains all the synsets which needs to be removed from the Hindi Wordnet\n",
    "with open(\"data/wordEmbs.txt\", 'r') as emb_word_f, open(\"data/sets2remove.txt\", 'w') as inspectf:\n",
    "    embedding_content = emb_word_f.read()\n",
    "    bwords = embedding_content.split(\"$\")\n",
    "    print(len(bwords))\n",
    "    # count = 0\n",
    "    # first creating two sets one set for all the sets which are going to be removed.\n",
    "    # second mapping for the sets which definitely have some words in the ball embeddings\n",
    "    wordsto_remove = []\n",
    "    for word, value in word2synsets.items():\n",
    "        normword = word.strip(\" \")\n",
    "        # print(normword)\n",
    "        # check if the words from wordnet are present in the embedding words\n",
    "        if normword in bwords:\n",
    "            # print(\"prs\")\n",
    "            # count += 1\n",
    "            for typ, lis in value.items():\n",
    "                for i in range(0, len(lis)):\n",
    "                    # why not save type\n",
    "                    to_keep.add(lis[i])\n",
    "        else:\n",
    "            wordsto_remove.append(word)\n",
    "            # print(\"abs\")\n",
    "            for typ, lis in value.items():\n",
    "                for i in range(0, len(lis)):\n",
    "                    to_remove.add(lis[i])\n",
    "\n",
    "    # removing the words from word2synsets Dictionary\n",
    "    # this ensure we don't persue paths for such words\n",
    "    for i in range(0, len(wordsto_remove)):\n",
    "        del word2synsets[wordsto_remove[i]]\n",
    "\n",
    "    # Ensuring sets that need not to be removed\n",
    "    amgsset = set()\n",
    "    for set in to_remove:\n",
    "        if set in to_keep:\n",
    "            amgsset.add(set)\n",
    "    # removing these amg sets from to_remove\n",
    "    # print(amgsset)\n",
    "    for set in amgsset:\n",
    "        to_remove.remove(set)\n",
    "\n",
    "    # if some set in to_keep by some word\n",
    "    inspectf.write(\" \".join(to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to print Paths\n",
    "* This method prints the leaf to parent path of given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printUptoRoot(sen, key, wordtype):\n",
    "    key_exp = {}\n",
    "    # path length\n",
    "    count = 0\n",
    "    tlen = 0\n",
    "    while (True):\n",
    "        if key not in key_exp:\n",
    "            key_exp[key] = True\n",
    "            sen += \"<-\" + str(key)\n",
    "            tlen += 1\n",
    "            # reminder take len1 words\n",
    "            if key in synset2hypes:\n",
    "                # check here wordtype not used\n",
    "                if int(wordtype) in synset2hypes[key].keys():\n",
    "                    lis = synset2hypes[key][int(wordtype)]\n",
    "                    key = lis[0]\n",
    "                    for i in range(0, len(lis)):\n",
    "                        # we could do the to_remove check here for better results\n",
    "                        if lis[i] not in to_remove:\n",
    "                            key = lis[i]\n",
    "                            break\n",
    "                else:\n",
    "                    print(\"Rare case!\")\n",
    "                    # reminder why not add them directly to the root\n",
    "                    for typ, lis in synset2hypes[key].items():\n",
    "                        lis = synset2hypes[key][int(typ)]\n",
    "                        key = lis[0]\n",
    "                        to_break = False\n",
    "                        for i in range(0, len(lis)):\n",
    "                            # we could do the to_remove check here for better results\n",
    "                            if lis[i] not in to_remove:\n",
    "                                to_break = True\n",
    "                                key = lis[i]\n",
    "                                break\n",
    "                        if to_break:\n",
    "                            break\n",
    "            else:\n",
    "                # Analyze this path and remove the sets which belongs to to_remove set\n",
    "                tokens = sen.split(\"<-\")\n",
    "                newsen = tokens[0]\n",
    "                for i in range(1, len(tokens)):\n",
    "                    if tokens[i] not in to_remove:\n",
    "                        newsen += \"<-\" + str(tokens[i])\n",
    "                    else:\n",
    "                        count += 1\n",
    "                if (tlen > 1):\n",
    "                    tree_struct.write(newsen + \"$\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing paths\n",
    "* This code will run through all the word and print their paths until no further parent is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = []\n",
    "sentence = \"\"\n",
    "num = 0\n",
    "modernSyn2Words = {}\n",
    "count = 0\n",
    "for word, value in word2synsets.items():\n",
    "    # num += 1\n",
    "    for typ, lis in value.items():\n",
    "        alpha = assign_alpha(typ)\n",
    "        for i in range(0, len(lis)):\n",
    "            # print(word)\n",
    "            wordversion = word + \".\" + alpha + \".\" + \"{:02d}\".format(i+1)\n",
    "\n",
    "            # if the set already exist in the modernSyn2Words just add this word also\n",
    "            # otherwise add new list with this word\n",
    "            if lis[i] in modernSyn2Words.keys():\n",
    "                modernSyn2Words[lis[i]].append(wordversion)\n",
    "            else:\n",
    "                modernSyn2Words[lis[i]] = [wordversion]\n",
    "\n",
    "            count += printUptoRoot(wordversion, lis[i], typ)\n",
    "            # printHch(sentence, lis[i], 0)\n",
    "            # wordList.append((key+alpha+str(i+1), lis[i]))\n",
    "    # if num == 1000:\n",
    "    #     break\n",
    "print(\"Total sets removed:{}\".format(count))\n",
    "tree_struct.write(\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing new set to word pairs\n",
    "* The below code prints the modernSyn2Words dictonary we created into the set2WordV.txt file so that it can be used in data generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/set2WordV.txt\", 'w') as s2w:\n",
    "    for set, values in modernSyn2Words.items():\n",
    "        s2w.write(str(set) + \":\" + values[0] + \"$\")\n",
    "    s2w.write(\"root:root\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

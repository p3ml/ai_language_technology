{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#0071BD;color:white;text-align:center;padding-top:0.8em;padding-bottom: 0.8em\">\n",
    "Word Embeddings - A Numerric Building Block of Natural Language Processing \n",
    "</h1>\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"background-color:#66A5D1;padding-top:0.2em;padding-bottom: 0.2em\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of this notebook\n",
    "\n",
    "Words can be represented by Vectors, so that co-occurrence relations among words in the corpus can be approximated \n",
    "through nummeric measurements, such as the cosine similarity, of their vector representations. \n",
    "\n",
    "In this notebook, we use `gensim` package to describe basic operations of word-embeddings. `Gensim` is a python package for topic modeling (LDA) in Natural Language Processing, and also provides a number of useful tools, such as word-embeddings. \n",
    "\n",
    "We first import `Word2Vec` from `gensim` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word2Vec` provides an easy-to-use interface to use `NLTK` corpora (Natural Language Toolkit). We import its movie review corpora and Brown corpoa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews, brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors representing word co-occurrence relations in movie reviews are easily imported by one Python line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewVec = Word2Vec(movie_reviews.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 most similar words of `way` in the movie review corpora are listed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('situation', 0.7207054495811462),\n",
       " ('stuff', 0.6776666641235352),\n",
       " ('material', 0.6655994653701782),\n",
       " ('audience', 0.661513090133667),\n",
       " ('money', 0.6551351547241211),\n",
       " ('viewer', 0.6523748636245728),\n",
       " ('thing', 0.6413513422012329),\n",
       " ('place', 0.6360496282577515),\n",
       " ('it', 0.6343892216682434),\n",
       " ('mind', 0.6280417442321777)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewVec.wv.most_similar('way', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the results of using Brown corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownVec = Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownVec.wv.most_similar('way', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-occurrence relations are determined (or biased) by the corpora. We can compare the results of using Brown corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewVec.wv.most_similar('brown', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the results of using Brown corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownVec.wv.most_similar('brown', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the different neighbors of the same word. In real applications, we need to train word embeddings of a particular corpora. It is also simple to do this. Suppose we have 5 sentences as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snts = [\n",
    "    ['why','do','so','many','egyptian','statues','have','broken','noses'],\n",
    "    ['why','are','the','statues','noses','broken'],\n",
    "    ['it','might','seem','inevitable','that','after','thousands','of','years','an','ancient','artifact','would','show','wear','and','tear'],\n",
    "    ['but','this','simple','observation','led','bleiberg','to','uncover','a','widespread','pattern','of','deliberate','destruction',],\n",
    "    ['which','pointed','to','a','complex','set','of','reasons','why','most','works','of','egyptian','art','came','to','be','defaced','in','the','first','place']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector representation of this 5 sentences can be easily trained by the following function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel = Word2Vec(snts, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the top-level features by just using the `print` function. `vocab=55` means that there are 55 words, `size=100` means that the vector size is 100, `alpha` is used for adjusting the training process, its default starting value is 0.025, and decreases linearly after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2vModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary can be accessed and printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w2vModel.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following formula to see the vector representation of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2vModel.wv['why'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following formula to see the top 10 words with the highest cosine similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel.wv.most_similar('noses', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this model into a file `myModel.bin`, and load this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel.save('myModel.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = Word2Vec.load('myModel.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better intuition on the co-occurrrence relations among all words, we can plot these vectors into a 2-dimensional space. We first need to reduce their dimensions from 100 to 2 by using `PCA` (Principal Component Analysis) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2D = pca.fit_transform(w2vModel.wv[w2vModel.wv.vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize these points, we use `pyplot` from the `matplotlib` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`matplotlib.rcParams['figure.figsize']` controlls the figure size, the default value is `[10, 5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(points2D[:, 0], points2D[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which word does a point represent? Let us tag them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(points2D[:, 0], points2D[:,1])\n",
    "for index, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(points2D[index,0], points2D[index,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us a function which accepts a http address as input, extracts all the sentences (tagged by the `contextClass` in the webpage) from the webpage pointed by the address, computes word embeddings, and save them into an output file, plot the result with tagged words.\n",
    "```\n",
    "def computing_word2vec_in_webpage(httpAddress   = 'http://...', \n",
    "                              contextClass  = '',\n",
    "                              outputW2VFile = '/home/user/.../w2vWeb.bin':\n",
    "    \n",
    "```\n",
    "We decompose this process into four steps:\n",
    "* extract the webpage pointed by the http address\n",
    "* transform these texts into a list of sentences, each sentence is a list of tokens\n",
    "* use gensim to train word embeddings\n",
    "* save the result into output file, and plot\n",
    "\n",
    "\n",
    "We use `urllib.request` pacakge to retrieve the webpage pointed by 'https://edition.cnn.com/style/article/egyptian-statues-broken-noses-artsy/index.html', and `BeautifulSoup` package to retrieve texts.\n",
    "\n",
    "### note: make sure that your computer is allowed to access and retrieve webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = ''\n",
    "httpAddress = 'https://edition.cnn.com/style/article/egyptian-statues-broken-noses-artsy/index.html'\n",
    "contextClass = 'Paragraph__component BasicArticle__paragraph BasicArticle__pad'\n",
    "webpage = urllib.request.urlopen(httpAddress).read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(webpage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value should not be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage, 'html.parser')\n",
    "sections = [sec.text for sec in soup.find_all(class_= contextClass)]\n",
    "corpora = \" \".join(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use NLTK tools to transform this text into a list of sentences, each sentence is a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCorpora = [tokenizer.tokenize(snt) for snt in nltk.sent_tokenize(corpora.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(inputCorpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel = Word2Vec(inputCorpora, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2vModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel.wv.most_similar('noses', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write up the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_word2vec_in_webpage(httpAddress   = 'http://...', \n",
    "                              contextClass  = 'p',\n",
    "                              outputW2VFile = 'w2vWeb.bin'):\n",
    "    # get raw corpus\n",
    "    import urllib\n",
    "    from bs4 import BeautifulSoup\n",
    "    webpage = urllib.request.urlopen(httpAddress).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    sections = [sec.text for sec in soup.find_all(class_= contextClass)]\n",
    "    corpora = \" \".join(sections)\n",
    "    \n",
    "    # pre-process raw corpus\n",
    "    import nltk\n",
    "    from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "    tokenizer = TweetTokenizer()\n",
    "    inputCorpora = [tokenizer.tokenize(snt) for snt in nltk.sent_tokenize(corpora.lower())]\n",
    "    \n",
    "    # the main machine learning process\n",
    "    from gensim.models import Word2Vec\n",
    "    w2vModel = Word2Vec(inputCorpora, min_count=1)\n",
    "    print(w2vModel)\n",
    "    \n",
    "    # save result\n",
    "    w2vModel.save(outputW2VFile)\n",
    "    \n",
    "    # visualize result\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    points2D = pca.fit_transform(w2vModel.wv[w2vModel.wv.vocab])\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['figure.figsize'] = [20, 10]\n",
    "    from matplotlib import pyplot\n",
    "    pyplot.scatter(points2D[:, 0], points2D[:,1])\n",
    "    pyplot.scatter(points2D[:, 0], points2D[:,1])\n",
    "    for index, word in enumerate(words):\n",
    "        pyplot.annotate(word, xy=(points2D[index,0], points2D[index,1]))\n",
    "    \n",
    "    \n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computing_word2vec_in_webpage(httpAddress = 'https://edition.cnn.com/style/article/egyptian-statues-broken-noses-artsy/index.html',\n",
    "                              contextClass = 'Paragraph__component BasicArticle__paragraph BasicArticle__pad',\n",
    "                              outputW2VFile = 'w2vEgyptianBrokenBoses.bin')                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "\n",
    "* Tomas Mikolov, Kai Chen, Greg Corrado,Jeffrey Dean (2013). Efficient Estimation of Word Representations in Vector Space. CoRR:abs/1301.3781. <a>http://arxiv.org/abs/1301.3781</a> \n",
    "* Jeffrey Pennington, Richard Socher,d Christopher D. Manning (2014). GloVe: Global Vectors for Word Representation. EMNLP-14. <a>https://nlp.stanford.edu/projects/glove/</a>\n",
    "* Omer Levy and Yoav Goldberg (2014). Dependency-Based Word Embeddings. ACL-14. <a>https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/2</a>\n",
    "\n",
    "<p style=\"background-color:#66A5D1;padding-top:0.2em;padding-bottom: 0.2em\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"1\" style=\"text-align:left;background-color:#0071BD;color:white\">\n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "            <img alt=\"Creative Commons License\" style=\"border-width:0;float:left;padding-right:10pt\"\n",
    "                 src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" />\n",
    "        </a>\n",
    "        &copy; T. Dong, C. Bauckhage<br/>\n",
    "        Licensed under a \n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" style=\"color:white\">\n",
    "            CC BY-NC 4.0\n",
    "        </a>.\n",
    "      </td>\n",
    "      <td colspan=\"2\" style=\"text-align:left;background-color:#66A5D1\">\n",
    "          <b>Acknowledgments:</b>\n",
    "          This material was prepared within the project\n",
    "          <a href=\"http://www.b-it-center.de/b-it-programmes/teaching-material/p3ml/\" style=\"color:black\">\n",
    "              P3ML\n",
    "          </a> \n",
    "          which is funded by the Ministry of Education and Research of Germany (BMBF)\n",
    "          under grant number 01/S17064. The authors gratefully acknowledge this support.\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Chinese N-Ball Embeddigs\n",
    "\n",
    "contributed by **Shichen Zhan**\n",
    "\n",
    "This instruction is written for the lab AI Language Technology (WiSe 2018) offered by the Fraunhofer IAIS. It is used for generating n-ball embedding files of Chinese language - contributed by **Shichen Zhan**.\n",
    "\n",
    "In this file, it constructs child and catcode file using the index in English-version wordnet(Princeton WordNet 3.1), translate corresponding English words into Chinese words,using the same relations as english words(hypernym and hyponym).\n",
    "Because the vocabularies of pre-trained word embeddings and Chinese wordnet are different, \n",
    "it is important to delete part  of words in each vocabulary, making them identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "we use two package, bs4 and nltk. Bs4 is for handling the file of Chinese wordnet(wn-cmn-lmf.xml) and nltk is for translating Chinese word into English word,in order to obtain their word relations(hypernym and hyponym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from nltk.corpus import wordnet as wn\n",
    "soup = bs(open(\"/home/zhanshichen/Desktop/code/wn-cmn-lmf.xml\"),\"xml\")\n",
    "pa_child = {}\n",
    "parent = {}\n",
    "tag = soup.LexicalEntry\n",
    "#all Chinese words which appearing in w2v.vector(file of pre-trained word embeddings)\n",
    "vector = []\n",
    "with open(\"/home/zhanshichen/nball4tree/w2v.vector\",\"r\") as f:\n",
    "    lines = f.readlines()     \n",
    "    for line in lines:\n",
    "        vec = line.split(\" \")\n",
    "        vector.append(vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a dictionary **pa_child** to store all Chinese word name obtained from the Chinese word net. Furtheremore, it will be used for recording their relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_name = []  \n",
    "#store all word-name which appearing in wordnet, in order to shearing the vector file.\n",
    "\n",
    "while not tag.name == \"Synset\":\n",
    "    tag_sense = tag.Sense\n",
    "    tag_id = tag_sense['synset']\n",
    "    tag_name_o = tag.Lemma['writtenForm']  \n",
    "    part = tag.Lemma['partOfSpeech']  \n",
    "    i = 1\n",
    "    number = str(i)\n",
    "\n",
    "    #remove \"+\" from tag_name\n",
    "    tag_name = tag_name_o.replace(\"+\",\"\")     \n",
    "\n",
    "\n",
    "    if (tag_name in vector) and (len(pa_child) < 10000):\n",
    "    # if tag_name in vector :\n",
    "        word_name.append(tag_name)\n",
    "        pa_child[tag_id] = {}\n",
    "\n",
    "\n",
    "        tag_total_name = tag_name + '.' + part + '.' + number\n",
    "        pa_child[tag_id][tag_total_name] = []\n",
    "\n",
    "\n",
    "        #if one word has more than one meanings\n",
    "        while tag_sense.next_sibling.next_sibling:\n",
    "            i = i + 1\n",
    "            number = str(i)\n",
    "            tag_sense = tag_sense.next_sibling.next_sibling\n",
    "            tag_id = tag_sense['synset']\n",
    "            pa_child[tag_id] = {}\n",
    "            tag_total_name = tag_name + '.' + part + '.' + number\n",
    "            pa_child[tag_id][tag_total_name] = []\n",
    "\n",
    "    tag = tag.next_sibling.next_sibling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create another dictionary, **parent**, to save the hypernym relations of words, through searching index in english-version wordnet and translating into corresponding Chinese words. Particularly, if a word has more than one hypernym words, we just choose one of them and ignore others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for id in pa_child: \n",
    "    b = id.split('-')\n",
    "    english_id =  b[2] + b[3]\n",
    "    #15028818n format\n",
    "\n",
    "    parent[id] = []\n",
    "    for name in pa_child[id]: \n",
    "        parent[id].append(name)\n",
    "\n",
    "    try:\n",
    "        english_name = wn.of2ss(english_id)  #english_name format ： Synset('isoagglutinin.n.01')\n",
    "    except:\n",
    "        continue  \n",
    "    else:\n",
    "\n",
    "        \n",
    "        children_names = english_name.hyponyms()\n",
    "        if children_names:  \n",
    "            for child_name in children_names:\n",
    "                child_id = str(child_name.offset()).zfill(8) + '-' + child_name.pos()\n",
    "                chinese_child_id = 'cmn-10-' + child_id\n",
    "                if chinese_child_id in pa_child.keys():\n",
    "                    for name in pa_child[id]:\n",
    "                        pa_child[id][name].append(chinese_child_id)\n",
    "\n",
    "        parent_names = english_name.hypernyms()  \n",
    "        if parent_names:  \n",
    "            label = 0\n",
    "            for parent_name in parent_names:\n",
    "\n",
    "\n",
    "                only_parent_name = parent_name  \n",
    "                only_parent_id = str(only_parent_name.offset()).zfill(8) + '-' + only_parent_name.pos()\n",
    "                chinese_parent_id = 'cmn-10-' + only_parent_id\n",
    "                if (chinese_parent_id in pa_child.keys()) and (label == 0):  \n",
    "                    parent[id].append(chinese_parent_id)\n",
    "                    label = 1\n",
    "                elif (chinese_parent_id in pa_child.keys()) and (label == 1) : \n",
    "                    \n",
    "                #if there are multiple parent word for one word, keep only one parent word and remove this relations in other's nodes.\n",
    "                    for delete_parent_name in pa_child[chinese_parent_id]:\n",
    "                        if id in pa_child[chinese_parent_id][delete_parent_name]:\n",
    "                            pa_child[chinese_parent_id][delete_parent_name].remove(id)\n",
    "#                             print(\"after delete from \",delete_parent_name,\" \",id)\n",
    "#                             print(\"then\",pa_child[chinese_parent_id][delete_parent_name])\n",
    "                            # print(\"delete relation 1 \\n\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check if each child-tree is a tree(not graph),in other words, its parent-node has been included in its children nodes. We realize it using depth-first traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for id in parent:\n",
    "    if len(parent[id]) == 1:  \n",
    "        parent_path = []\n",
    "        # parent_path.append(id)  \n",
    "        for x in pa_child[id]:  \n",
    "            name = x\n",
    "        node_id = id\n",
    "\n",
    "        queue=[]\n",
    "        queue.append(node_id)    \n",
    "        while queue:\n",
    "            v = queue.pop()    \n",
    "            parent_path.append(v) \n",
    "            for x in pa_child[v]:\n",
    "                name = x\n",
    "            for children_id in reversed(pa_child[v][name]):   \n",
    "                if children_id in pa_child.keys():\n",
    "                    if children_id in parent_path:\n",
    "                        pa_child[v][name].remove(children_id)\n",
    "                        print(\"deleted 1 \\n\")\n",
    "                    else:\n",
    "                        queue.append(children_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate **child_trans.txt**, recording parent-children relations among word-senses. If a word in **parent** dic does not have parent nodes, we take **root** node as its parent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/zhanshichen/child_trans.txt\",\"w\") as f:\n",
    "                            \n",
    "    f.write(\"*root* \")\n",
    "    for id in parent:\n",
    "        if len(parent[id]) == 1 :\n",
    "                f.write(parent[id][0]+' ')   #写入\n",
    "                \n",
    "                parent[id].append(\"*root*\")  \n",
    "                # count = count + 1\n",
    "    f.write('\\n')    \n",
    "    \n",
    "    for pa_id in pa_child:\n",
    "        for pa_name in pa_child[pa_id]:\n",
    "            # if pa_name in half_wordname:    \n",
    "            f.write(pa_name + ' ')\n",
    "            for children_id in pa_child[pa_id][pa_name]:  \n",
    "                if children_id in pa_child.keys():\n",
    "                    for children_name in pa_child[children_id]:\n",
    "                        f.write(children_name)\n",
    "                        f.write(\" \")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to re-create file of pre-trained word embeddings(w2v.vector), in order to match the vocabulary in wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/home/zhanshichen/nball4tree/w2v.vector\",\"r\") as f:\n",
    "    with open(\"/home/zhanshichen/w2v_new_trans.txt\",\"w\") as f_w:    \n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            vec = line.strip().split(\" \")\n",
    "            if vec[0] in word_name:\n",
    "                for each_vector in vec:\n",
    "                    f_w.write(each_vector + \" \")\n",
    "            f_w.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use catcode_trans.txt to store the parent location code of a word-sense in the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "longest_dimension = 0\n",
    "with open(\"/home/zhanshichen/catcode_trans.txt\",\"w\") as f:\n",
    "    for id in parent:  \n",
    "        # if parent[id][0] in half_wordname:  \n",
    "        f.write(parent[id][0]+' ')   \n",
    "        node_id = id\n",
    "        position = []  \n",
    "        \n",
    "        while not parent[node_id][1] == \"*root*\":  \n",
    "            parent_id =  parent[node_id][1]\n",
    "            number = 0\n",
    "            for x in pa_child[parent_id]:\n",
    "                for child_id in pa_child[parent_id][x]:\n",
    "                    number = number + 1\n",
    "                    if child_id == node_id:\n",
    "                        position.append(str(number) + \" \")\n",
    "                        break\n",
    "            node_id = parent_id  \n",
    "            if len(position) == 5:  #deepest tree\n",
    "                print(\"root node5:\" + parent[node_id][0])\n",
    "\n",
    "            \n",
    "        position.append(\"1\")\n",
    "        dimension = len(position)\n",
    "        if dimension > longest_dimension:\n",
    "            longest_dimension = dimension\n",
    "\n",
    "        level = 0 \n",
    "        for po_number in position[::-1]:\n",
    "            f.write(po_number + ' ')\n",
    "            level = level + 1\n",
    "        while level < 9:\n",
    "            f.write(\"0\" + \" \")\n",
    "            level = level + 1\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remove space between lines in the new generated word pre-trained word embedding file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/zhanshichen/nball4tree/w2v_new_trans.txt\",\"r\") as f_r:\n",
    "    with open(\"/home/zhanshichen/w2v_2.txt\",\"w\") as f_w:\n",
    "        for line in f_r.readlines():                                  \n",
    "            data=line.strip()\n",
    "            if len(data)!=0:\n",
    "                f_w.write(data)\n",
    "                f_w.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "The training method references [nball4tree](https://github.com/gnodisnait/nball4tree), where it trains and evaluates nball embedding files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ git clone https://github.com/gnodisnait/nball4tree.git\n",
    "$ cd nball4tree\n",
    "$ virtualenv venv\n",
    "$ source venv/bin/activate\n",
    "$ pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$python nball.py --train_nball nball.txt --w2v w2v_2.txt  --ws_child child_trans.txt  --ws_catcode catcode_trans.txt  --log log.txt\n",
    "% --train_nball: output file of nball embeddings\n",
    "% --w2v_2.txt: file of pre-trained word embeddings\n",
    "% --child_trans.txt: file of parent-children relations among word-senses\n",
    "% --catcode_trans.txt: file of the parent location code of a word-sense in the tree structure\n",
    "% --log.txt: log file, shall be located in the same directory as the file of nball embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several hours, it will generate **nball.txt** and print the result of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "We successfully generated the parent-child relations, catcode and wordnet files for Chinese language. Applying these to nball embedding, we obtain the nball-embedding file which records all tree structures and relations.\n",
    "### the result of experiment\n",
    "| Name | Result|\n",
    "| --- | --- |\n",
    "| parent-children relations among word-senses | [child_trans.txt](https://drive.google.com/open?id=1MLveoPRB4JN4HJF01a3cXWtZ-ox27JWL) |\n",
    "|parent location code|[catcode_trans.txt](https://drive.google.com/open?id=1JI_UUbse-oJtdumfGy6wJxJOH_uqjuNJ)|\n",
    "| pre-trained word embeddings|[w2v_2.txt](https://drive.google.com/open?id=1FGb6eIapn8VA_NafSzFRpP-cAigcQ4hf)|\n",
    "|log file|[training.log](https://drive.google.com/open?id=1xHEgiq60YMW_V8Sf5tVTHCrpilurrAsI)|\n",
    "|nball word embedding|[nball.txt](https://drive.google.com/open?id=1yqOyzzq6N_b54IU-tZCPcrLF90Od6Rcv)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "      <td colspan=\"1\" style=\"text-align:left;background-color:#0071BD;color:white\">\n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">\n",
    "            <img alt=\"Creative Commons License\" style=\"border-width:0;float:left;padding-right:10pt\"\n",
    "                 src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" />\n",
    "        </a>\n",
    "        &copy; T. Dong, C. Bauckhage<br/>\n",
    "        Licensed under a \n",
    "        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" style=\"color:white\">\n",
    "            CC BY-NC 4.0\n",
    "        </a>.\n",
    "      </td>\n",
    "      <td colspan=\"2\" style=\"text-align:left;background-color:#66A5D1\">\n",
    "          <b>Acknowledgments:</b>\n",
    "          This material was prepared within the project\n",
    "          <a href=\"http://www.b-it-center.de/b-it-programmes/teaching-material/p3ml/\" style=\"color:black\">\n",
    "              P3ML\n",
    "          </a> \n",
    "          which is funded by the Ministry of Education and Research of Germany (BMBF)\n",
    "          under grant number 01/S17064. The authors gratefully acknowledge this support.\n",
    "      </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Embedding\n",
    "\n",
    "A utility to load word embedding model.\n",
    "\n",
    "**AraVec N-Gram** model is used as a source of word embeddings, as it provides larger set of embeddings to the words we have in **AraWordNet**. **fastText** and **AraVec uni-gram** are both uni-gram models so they are missing many words in the WordNet. In this notebook, we load the the N-gram model.\n",
    "\n",
    "### Prerequisite:\n",
    "- Define `fasttext_path`, `uni_gram_aravec_path` and `n_gram_aravec_path` variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import gensim\n",
    "\n",
    "%store -r fasttext_path\n",
    "%store -r uni_gram_aravec_path\n",
    "%store -r n_gram_aravec_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fastText word embedding model\n",
    "\n",
    "fastText[1] word embedding could be found at https://fasttext.cc/docs/en/crawl-vectors.html. We use the Arabic word embedding.\n",
    "\n",
    "[1] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, “Learning Word Vectors for 157 Languages”, in Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018), 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load the model, the code is imported from fasttext: https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data\n",
    "\n",
    "fasttext_model = load_vectors(fasttext_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load AraVec Uni-Gram word embedding model\n",
    "\n",
    "AraVec[2] uni-gram word embedding could be found at https://github.com/bakrianoo/aravec#unigrams-models. We use the Wikipedia-SkipGram with vector size 300.\n",
    "\n",
    "[2] A. Soliman, K. Eisa, and S. R. El-Beltagy, “AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP”, in proceedings of the 3rd International Conference on Arabic Computational Linguistics (ACLing 2017), Dubai, UAE, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_aravec_model = gensim.models.Word2Vec.load(uni_gram_aravec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load AraVec N-Gram word embedding model\n",
    "\n",
    "AraVec n-gram word embedding could be found at https://github.com/bakrianoo/aravec#n-grams-models-1. We use the Wikipedia-SkipGram with vector size 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_aravec_model = gensim.models.Word2Vec.load(n_gram_aravec_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
